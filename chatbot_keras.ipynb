{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    df = pd.read_csv(filename, encoding = \"latin1\", names = [\"Sentence\", \"Intent\"])\n",
    "    print(df.head())\n",
    "    intent = df[\"Intent\"]\n",
    "    unique_intent = list(set(intent))\n",
    "    sentences = list(df[\"Sentence\"])\n",
    "  \n",
    "    return (intent, unique_intent, sentences)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Sentence   Intent\n",
      "0  How many <item> are left?  balance\n",
      "1           Stock of <Item>?  balance\n",
      "2        Do you have <Item>?  balance\n",
      "3         Balance of <Item>?  balance\n",
      "4           Any <Item> left?  balance\n"
     ]
    }
   ],
   "source": [
    "intent, unique_intent, sentences = load_dataset(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How many <item> are left?', 'Stock of <Item>?', 'Do you have <Item>?', 'Balance of <Item>?', 'Any <Item> left?']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/axelalvarsson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/axelalvarsson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define stemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(sentences):\n",
    "    words = []\n",
    "    for s in sentences:\n",
    "        clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", s)\n",
    "        w = word_tokenize(clean)\n",
    "        #stemming\n",
    "        words.append([i.lower() for i in w])\n",
    "    \n",
    "    return words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "[['how', 'many', 'item', 'are', 'left'], ['stock', 'of', 'item']]\n"
     ]
    }
   ],
   "source": [
    "cleaned_words = cleaning(sentences)\n",
    "print(len(cleaned_words))\n",
    "print(cleaned_words[:2])  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(words, filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'):\n",
    "    token = Tokenizer(filters = filters)\n",
    "    token.fit_on_texts(words)\n",
    "    print(token)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(words):\n",
    "    return(len(max(words, key = len)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7fe34a868ac8>\n",
      "Vocab Size = 82 and Maximum length = 9\n"
     ]
    }
   ],
   "source": [
    "word_tokenizer = create_tokenizer(cleaned_words)\n",
    "vocab_size = len(word_tokenizer.word_index) + 1\n",
    "max_length = max_length(cleaned_words)\n",
    "\n",
    "print(\"Vocab Size = %d and Maximum length = %d\" % (vocab_size, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_doc(token, words):\n",
    "    return(token.texts_to_sequences(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_doc = encoding_doc(word_tokenizer, cleaned_words)\n",
    "#print(encoded_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_doc(encoded_doc, max_length):\n",
    "    return(pad_sequences(encoded_doc, maxlen = max_length, padding = \"post\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_doc = padding_doc(encoded_doc, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3, 22,  1,  7, 13,  0,  0,  0,  0],\n",
       "       [ 9,  4,  1,  0,  0,  0,  0,  0,  0],\n",
       "       [14,  2, 28,  1,  0,  0,  0,  0,  0],\n",
       "       [34,  4,  1,  0,  0,  0,  0,  0,  0],\n",
       "       [29,  1, 13,  0,  0,  0,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_doc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded docs =  (118, 9)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of padded docs = \",padded_doc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7fe3646f7550>\n"
     ]
    }
   ],
   "source": [
    "#tokenizer with filter changed\n",
    "output_tokenizer = create_tokenizer(unique_intent, filters = '!\"#$%&()*+,-/:;<=>?@[\\]^`{|}~')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'price': 1, 'order': 2, 'balance': 3, 'common': 4}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_output = encoding_doc(output_tokenizer, intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_output = np.array(encoded_output).reshape(len(encoded_output), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(encode):\n",
    "    o = OneHotEncoder(sparse = False)\n",
    "    return(o.fit_transform(encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/axelalvarsson/opt/anaconda3/envs/nnlm/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "output_one_hot = one_hot(encoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_Y, val_Y = train_test_split(padded_doc, output_one_hot, shuffle = True, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X = (94, 9) and train_Y = (94, 4)\n",
      "Shape of val_X = (24, 9) and val_Y = (24, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train_X = %s and train_Y = %s\" % (train_X.shape, train_Y.shape))\n",
    "print(\"Shape of val_X = %s and val_Y = %s\" % (val_X.shape, val_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 128, input_length = max_length, trainable = True))\n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "#   model.add(LSTM(128))\n",
    "    model.add(Dense(32, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4, activation = \"softmax\"))\n",
    "  \n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 9, 128)            10496     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 282,020\n",
      "Trainable params: 282,020\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size, max_length)\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/axelalvarsson/opt/anaconda3/envs/nnlm/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 94 samples, validate on 24 samples\n",
      "Epoch 1/200\n",
      "94/94 [==============================] - 4s 47ms/step - loss: 1.3855 - accuracy: 0.2660 - val_loss: 1.3766 - val_accuracy: 0.3333\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.37661, saving model to model.h5\n",
      "Epoch 2/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 1.3789 - accuracy: 0.1915 - val_loss: 1.3656 - val_accuracy: 0.4167\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.37661 to 1.36564, saving model to model.h5\n",
      "Epoch 3/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 1.3655 - accuracy: 0.3723 - val_loss: 1.3542 - val_accuracy: 0.5417\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.36564 to 1.35423, saving model to model.h5\n",
      "Epoch 4/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 1.3611 - accuracy: 0.3298 - val_loss: 1.3386 - val_accuracy: 0.4167\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.35423 to 1.33859, saving model to model.h5\n",
      "Epoch 5/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 1.3369 - accuracy: 0.4574 - val_loss: 1.3171 - val_accuracy: 0.4583\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.33859 to 1.31709, saving model to model.h5\n",
      "Epoch 6/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 1.3108 - accuracy: 0.5000 - val_loss: 1.2864 - val_accuracy: 0.4167\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.31709 to 1.28640, saving model to model.h5\n",
      "Epoch 7/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 1.2805 - accuracy: 0.4574 - val_loss: 1.2443 - val_accuracy: 0.4583\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.28640 to 1.24431, saving model to model.h5\n",
      "Epoch 8/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 1.2364 - accuracy: 0.5638 - val_loss: 1.1872 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.24431 to 1.18720, saving model to model.h5\n",
      "Epoch 9/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 1.1824 - accuracy: 0.6064 - val_loss: 1.1088 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.18720 to 1.10882, saving model to model.h5\n",
      "Epoch 10/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 1.0578 - accuracy: 0.6809 - val_loss: 0.9797 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.10882 to 0.97966, saving model to model.h5\n",
      "Epoch 11/200\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.9502 - accuracy: 0.6702 - val_loss: 0.8269 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.97966 to 0.82688, saving model to model.h5\n",
      "Epoch 12/200\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.8326 - accuracy: 0.7021 - val_loss: 0.6767 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.82688 to 0.67671, saving model to model.h5\n",
      "Epoch 13/200\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.6192 - accuracy: 0.8511 - val_loss: 0.5843 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.67671 to 0.58434, saving model to model.h5\n",
      "Epoch 14/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.4965 - accuracy: 0.7872 - val_loss: 0.5261 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.58434 to 0.52611, saving model to model.h5\n",
      "Epoch 15/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.3059 - accuracy: 0.8936 - val_loss: 0.5013 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.52611 to 0.50126, saving model to model.h5\n",
      "Epoch 16/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2731 - accuracy: 0.9255 - val_loss: 0.4917 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.50126 to 0.49168, saving model to model.h5\n",
      "Epoch 17/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.2705 - accuracy: 0.8830 - val_loss: 0.5419 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.49168\n",
      "Epoch 18/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1612 - accuracy: 0.9362 - val_loss: 0.5842 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.49168\n",
      "Epoch 19/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0868 - accuracy: 0.9787 - val_loss: 0.5676 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.49168\n",
      "Epoch 20/200\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0790 - accuracy: 0.9894 - val_loss: 0.5914 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.49168\n",
      "Epoch 21/200\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 0.0890 - accuracy: 0.9468 - val_loss: 0.5724 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.49168\n",
      "Epoch 22/200\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0540 - accuracy: 1.0000 - val_loss: 0.6202 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.49168\n",
      "Epoch 23/200\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 0.0507 - accuracy: 0.9681 - val_loss: 0.6270 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.49168\n",
      "Epoch 24/200\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 0.0795 - accuracy: 0.9574 - val_loss: 0.7213 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.49168\n",
      "Epoch 25/200\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0652 - accuracy: 0.9787 - val_loss: 0.8203 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.49168\n",
      "Epoch 26/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0638 - accuracy: 0.9787 - val_loss: 0.7126 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.49168\n",
      "Epoch 27/200\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0363 - accuracy: 1.0000 - val_loss: 0.5696 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.49168\n",
      "Epoch 28/200\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 0.0409 - accuracy: 0.9787 - val_loss: 0.5933 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.49168\n",
      "Epoch 29/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0486 - accuracy: 0.9894 - val_loss: 0.6245 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.49168\n",
      "Epoch 30/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9681 - val_loss: 0.6421 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.49168\n",
      "Epoch 31/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.9894 - val_loss: 0.6416 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.49168\n",
      "Epoch 32/200\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0347 - accuracy: 0.9894 - val_loss: 0.6455 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.49168\n",
      "Epoch 33/200\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.6456 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.49168\n",
      "Epoch 34/200\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 0.0194 - accuracy: 1.0000 - val_loss: 0.6436 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.49168\n",
      "Epoch 35/200\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 0.0478 - accuracy: 0.9787 - val_loss: 0.6172 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.49168\n",
      "Epoch 36/200\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 0.0581 - accuracy: 0.9681 - val_loss: 0.5731 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.49168\n",
      "Epoch 37/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 1.0000 - val_loss: 0.5774 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.49168\n",
      "Epoch 38/200\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0286 - accuracy: 0.9894 - val_loss: 0.6118 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.49168\n",
      "Epoch 39/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.6411 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.49168\n",
      "Epoch 40/200\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 0.0302 - accuracy: 0.9894 - val_loss: 0.6725 - val_accuracy: 0.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_loss did not improve from 0.49168\n",
      "Epoch 41/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.7082 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.49168\n",
      "Epoch 42/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.7418 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.49168\n",
      "Epoch 43/200\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0159 - accuracy: 0.9894 - val_loss: 0.7545 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.49168\n",
      "Epoch 44/200\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0423 - accuracy: 0.9787 - val_loss: 0.7766 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.49168\n",
      "Epoch 45/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0385 - accuracy: 0.9894 - val_loss: 0.7968 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.49168\n",
      "Epoch 46/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.8178 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.49168\n",
      "Epoch 47/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.8392 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.49168\n",
      "Epoch 48/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.8552 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.49168\n",
      "Epoch 49/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.8613 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.49168\n",
      "Epoch 50/200\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 0.9894 - val_loss: 0.8631 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.49168\n",
      "Epoch 51/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.8709 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.49168\n",
      "Epoch 52/200\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0326 - accuracy: 0.9894 - val_loss: 0.8358 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.49168\n",
      "Epoch 53/200\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0186 - accuracy: 0.9894 - val_loss: 0.7858 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.49168\n",
      "Epoch 54/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0156 - accuracy: 0.9894 - val_loss: 0.7392 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.49168\n",
      "Epoch 55/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.9787 - val_loss: 0.7466 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.49168\n",
      "Epoch 56/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.8536 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.49168\n",
      "Epoch 57/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0381 - accuracy: 0.9787 - val_loss: 1.0115 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.49168\n",
      "Epoch 58/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0383 - accuracy: 0.9787 - val_loss: 0.8975 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.49168\n",
      "Epoch 59/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0443 - accuracy: 0.9787 - val_loss: 0.7687 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.49168\n",
      "Epoch 60/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0278 - accuracy: 0.9894 - val_loss: 0.7177 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.49168\n",
      "Epoch 61/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.8315 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.49168\n",
      "Epoch 62/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1057 - accuracy: 0.9681 - val_loss: 0.8207 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.49168\n",
      "Epoch 63/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0162 - accuracy: 0.9894 - val_loss: 0.7997 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.49168\n",
      "Epoch 64/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.7485 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.49168\n",
      "Epoch 65/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1021 - accuracy: 0.9681 - val_loss: 0.7229 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.49168\n",
      "Epoch 66/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.7687 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.49168\n",
      "Epoch 67/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.9513 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.49168\n",
      "Epoch 68/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0295 - accuracy: 0.9894 - val_loss: 0.8700 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.49168\n",
      "Epoch 69/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.8431 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.49168\n",
      "Epoch 70/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.8006 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.49168\n",
      "Epoch 71/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0195 - accuracy: 1.0000 - val_loss: 0.7692 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.49168\n",
      "Epoch 72/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0573 - accuracy: 0.9787 - val_loss: 0.7380 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.49168\n",
      "Epoch 73/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0350 - accuracy: 0.9894 - val_loss: 0.7218 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.49168\n",
      "Epoch 74/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.7118 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.49168\n",
      "Epoch 75/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.7053 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.49168\n",
      "Epoch 76/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.6971 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.49168\n",
      "Epoch 77/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6897 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.49168\n",
      "Epoch 78/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.6851 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.49168\n",
      "Epoch 79/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.6806 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.49168\n",
      "Epoch 80/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.6710 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.49168\n",
      "Epoch 81/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.6651 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.49168\n",
      "Epoch 82/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.6693 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.49168\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.6864 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.49168\n",
      "Epoch 84/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0185 - accuracy: 0.9894 - val_loss: 0.7125 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.49168\n",
      "Epoch 85/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0258 - accuracy: 0.9787 - val_loss: 0.7399 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.49168\n",
      "Epoch 86/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0185 - accuracy: 0.9894 - val_loss: 0.7685 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.49168\n",
      "Epoch 87/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0182 - accuracy: 0.9894 - val_loss: 0.7982 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.49168\n",
      "Epoch 88/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.8225 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.49168\n",
      "Epoch 89/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 0.9894 - val_loss: 0.8387 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.49168\n",
      "Epoch 90/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.8622 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.49168\n",
      "Epoch 91/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.8944 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.49168\n",
      "Epoch 92/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.9237 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.49168\n",
      "Epoch 93/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.9894 - val_loss: 0.9397 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.49168\n",
      "Epoch 94/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.9496 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.49168\n",
      "Epoch 95/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 0.9894 - val_loss: 0.9524 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.49168\n",
      "Epoch 96/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.9506 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.49168\n",
      "Epoch 97/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.9514 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.49168\n",
      "Epoch 98/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0175 - accuracy: 0.9894 - val_loss: 0.9513 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.49168\n",
      "Epoch 99/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.9503 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.49168\n",
      "Epoch 100/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.9519 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.49168\n",
      "Epoch 101/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.9539 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.49168\n",
      "Epoch 102/200\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.9698 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.49168\n",
      "Epoch 103/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.9788 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.49168\n",
      "Epoch 104/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0137 - accuracy: 0.9894 - val_loss: 0.9799 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.49168\n",
      "Epoch 105/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.9824 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.49168\n",
      "Epoch 106/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0305 - accuracy: 0.9787 - val_loss: 1.0123 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.49168\n",
      "Epoch 107/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 1.0781 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.49168\n",
      "Epoch 108/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 3.0350e-04 - accuracy: 1.0000 - val_loss: 1.1298 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.49168\n",
      "Epoch 109/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 1.1570 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.49168\n",
      "Epoch 110/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 1.1739 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.49168\n",
      "Epoch 111/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 1.0000 - val_loss: 1.1840 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.49168\n",
      "Epoch 112/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 1.1799 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.49168\n",
      "Epoch 113/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.1786 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.49168\n",
      "Epoch 114/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 9.3649e-04 - accuracy: 1.0000 - val_loss: 1.1846 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.49168\n",
      "Epoch 115/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 1.1938 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.49168\n",
      "Epoch 116/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0258 - accuracy: 0.9894 - val_loss: 1.1979 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.49168\n",
      "Epoch 117/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 0.9894 - val_loss: 1.2021 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.49168\n",
      "Epoch 118/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 1.2031 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.49168\n",
      "Epoch 119/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0313 - accuracy: 0.9787 - val_loss: 1.1944 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.49168\n",
      "Epoch 120/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.1837 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.49168\n",
      "Epoch 121/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.1860 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.49168\n",
      "Epoch 122/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 1.1516 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.49168\n",
      "Epoch 123/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0163 - accuracy: 0.9894 - val_loss: 1.1331 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.49168\n",
      "Epoch 124/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 0.9894 - val_loss: 1.1144 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.49168\n",
      "Epoch 125/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 1.1005 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.49168\n",
      "Epoch 126/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.0832 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.49168\n",
      "Epoch 127/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 5.6686e-04 - accuracy: 1.0000 - val_loss: 1.0708 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.49168\n",
      "Epoch 128/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.0660 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.49168\n",
      "Epoch 129/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 0.9894 - val_loss: 1.0668 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.49168\n",
      "Epoch 130/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 1.0667 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.49168\n",
      "Epoch 131/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 1.0643 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.49168\n",
      "Epoch 132/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.0493 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.49168\n",
      "Epoch 133/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.0389 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.49168\n",
      "Epoch 134/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.0399 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.49168\n",
      "Epoch 135/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.0570 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.49168\n",
      "Epoch 136/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.0684 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.49168\n",
      "Epoch 137/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.0765 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.49168\n",
      "Epoch 138/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.0827 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.49168\n",
      "Epoch 139/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.0872 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.49168\n",
      "Epoch 140/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.0905 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.49168\n",
      "Epoch 141/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.0948 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.49168\n",
      "Epoch 142/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 0.9894 - val_loss: 1.0728 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.49168\n",
      "Epoch 143/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 1.0534 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.49168\n",
      "Epoch 144/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 1.0544 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.49168\n",
      "Epoch 145/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0132 - accuracy: 0.9894 - val_loss: 1.0668 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.49168\n",
      "Epoch 146/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 1.0756 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.49168\n",
      "Epoch 147/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 0.9894 - val_loss: 1.0836 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.49168\n",
      "Epoch 148/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.9894 - val_loss: 1.0921 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.49168\n",
      "Epoch 149/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.1037 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.49168\n",
      "Epoch 150/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.1086 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.49168\n",
      "Epoch 151/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.9894 - val_loss: 1.1153 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.49168\n",
      "Epoch 152/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.1273 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.49168\n",
      "Epoch 153/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0145 - accuracy: 0.9894 - val_loss: 1.1373 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.49168\n",
      "Epoch 154/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.9787 - val_loss: 1.1380 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.49168\n",
      "Epoch 155/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.1434 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.49168\n",
      "Epoch 156/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.1472 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.49168\n",
      "Epoch 157/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 1.1579 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.49168\n",
      "Epoch 158/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 1.1638 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.49168\n",
      "Epoch 159/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 0.9894 - val_loss: 1.1562 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.49168\n",
      "Epoch 160/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.1509 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.49168\n",
      "Epoch 161/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.1463 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.49168\n",
      "Epoch 162/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.1424 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.49168\n",
      "Epoch 163/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 5.7339e-04 - accuracy: 1.0000 - val_loss: 1.1421 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.49168\n",
      "Epoch 164/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.1428 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.49168\n",
      "Epoch 165/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.1486 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.49168\n",
      "Epoch 166/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0185 - accuracy: 0.9894 - val_loss: 1.1732 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.49168\n",
      "Epoch 167/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.1900 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.49168\n",
      "Epoch 168/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.2020 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.49168\n",
      "Epoch 169/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9894 - val_loss: 1.2179 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.49168\n",
      "Epoch 170/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 1.2309 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.49168\n",
      "Epoch 171/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0135 - accuracy: 0.9894 - val_loss: 1.2422 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.49168\n",
      "Epoch 172/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0121 - accuracy: 0.9894 - val_loss: 1.2488 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.49168\n",
      "Epoch 173/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.2505 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.49168\n",
      "Epoch 174/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0229 - accuracy: 0.9894 - val_loss: 1.2605 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.49168\n",
      "Epoch 175/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.2629 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.49168\n",
      "Epoch 176/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 1.2480 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.49168\n",
      "Epoch 177/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 1.2334 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.49168\n",
      "Epoch 178/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.2230 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.49168\n",
      "Epoch 179/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 1.2304 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.49168\n",
      "Epoch 180/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 5.5445e-04 - accuracy: 1.0000 - val_loss: 1.2357 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.49168\n",
      "Epoch 181/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 1.2396 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.49168\n",
      "Epoch 182/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 1.2400 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.49168\n",
      "Epoch 183/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 5.0236e-04 - accuracy: 1.0000 - val_loss: 1.2362 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.49168\n",
      "Epoch 184/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.2334 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.49168\n",
      "Epoch 185/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.2346 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.49168\n",
      "Epoch 186/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 1.5248e-04 - accuracy: 1.0000 - val_loss: 1.2362 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.49168\n",
      "Epoch 187/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 1.2293 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.49168\n",
      "Epoch 188/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.2241 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.49168\n",
      "Epoch 189/200\n",
      "94/94 [==============================] - 2s 16ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.2217 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.49168\n",
      "Epoch 190/200\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.2193 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.49168\n",
      "Epoch 191/200\n",
      "94/94 [==============================] - 1s 13ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.2163 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.49168\n",
      "Epoch 192/200\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 9.3390e-04 - accuracy: 1.0000 - val_loss: 1.2205 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.49168\n",
      "Epoch 193/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.2389 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.49168\n",
      "Epoch 194/200\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.2604 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.49168\n",
      "Epoch 195/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 2.6067e-04 - accuracy: 1.0000 - val_loss: 1.2781 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.49168\n",
      "Epoch 196/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.2937 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.49168\n",
      "Epoch 197/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0276 - accuracy: 0.9787 - val_loss: 1.3102 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.49168\n",
      "Epoch 198/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 2.5185e-04 - accuracy: 1.0000 - val_loss: 1.3170 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.49168\n",
      "Epoch 199/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 0.9894 - val_loss: 1.2942 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.49168\n",
      "Epoch 200/200\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 5.2341e-04 - accuracy: 1.0000 - val_loss: 1.2615 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.49168\n"
     ]
    }
   ],
   "source": [
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "history = model.fit(train_X, train_Y, epochs = 200, batch_size = 32, validation_data = (val_X, val_Y), callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/axelalvarsson/opt/anaconda3/envs/nnlm/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    " model = load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(text):\n",
    "    clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", text)\n",
    "    test_word = word_tokenize(clean)\n",
    "    test_word = [w.lower() for w in test_word]\n",
    "    test_ls = word_tokenizer.texts_to_sequences(test_word)\n",
    "    print(test_word)\n",
    "    #Check for unknown words\n",
    "    if [] in test_ls:\n",
    "        test_ls = list(filter(None, test_ls))\n",
    "    \n",
    "    test_ls = np.array(test_ls).reshape(1, len(test_ls))\n",
    " \n",
    "    x = padding_doc(test_ls, max_length)\n",
    "  \n",
    "    pred = model.predict_proba(x)\n",
    "  \n",
    "  \n",
    "    return pred\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_output(pred, classes):\n",
    "    predictions = pred[0]\n",
    " \n",
    "    classes = np.array(classes)\n",
    "    ids = np.argsort(-predictions)\n",
    "    classes = classes[ids]\n",
    "    predictions = -np.sort(-predictions)\n",
    " \n",
    "    for i in range(pred.shape[1]):\n",
    "        print(\"%s has confidence = %s\" % (classes[i], (predictions[i])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'much', 'does', 'it', 'cost']\n",
      "price has confidence = 0.9295274\n",
      "balance has confidence = 0.070433706\n",
      "order has confidence = 3.834743e-05\n",
      "common has confidence = 5.215025e-07\n"
     ]
    }
   ],
   "source": [
    "#Price\n",
    "text = \"How much does it cost?\"\n",
    "pred = predictions(text)\n",
    "get_final_output(pred, unique_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'many', 'pants', 'are', 'in', 'stock']\n",
      "balance has confidence = 0.98781675\n",
      "price has confidence = 0.012177299\n",
      "common has confidence = 5.8738033e-06\n",
      "order has confidence = 8.942432e-08\n"
     ]
    }
   ],
   "source": [
    "#Balance\n",
    "text = \"how many pants are in stock?\"\n",
    "pred = predictions(text)\n",
    "get_final_output(pred, unique_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'want', 'to', 'buy', '3', 'pants']\n",
      "order has confidence = 0.89656526\n",
      "price has confidence = 0.08742738\n",
      "common has confidence = 0.008532433\n",
      "balance has confidence = 0.007474897\n"
     ]
    }
   ],
   "source": [
    "#Order\n",
    "text = \"I want to buy 3 pants\"\n",
    "pred = predictions(text)\n",
    "get_final_output(pred, unique_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'need', 'help']\n",
      "common has confidence = 0.9746465\n",
      "order has confidence = 0.017818842\n",
      "price has confidence = 0.0052660275\n",
      "balance has confidence = 0.0022686257\n"
     ]
    }
   ],
   "source": [
    "#Common\n",
    "text = \"I need help\"\n",
    "pred = predictions(text)\n",
    "get_final_output(pred, unique_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "opening_lines = ['Hello, what can I help you with?',\n",
    "                'How can I help you?',\n",
    "                'Whats on your mind today?']\n",
    "\n",
    "def randomOpening():\n",
    "    index = random.randint(0,len(opening_lines)-1)\n",
    "    print(opening_lines[index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, what can I help you with?\n"
     ]
    }
   ],
   "source": [
    "def cli():\n",
    "    while True:\n",
    "        randomOpening()\n",
    "        user_input = input(\"Ask the chatbot something: \")\n",
    "        print(user_input)\n",
    "        if(user_input==\"quit\"):\n",
    "            break\n",
    "        else:\n",
    "            pred = predictions(user_input)\n",
    "            print(pred)\n",
    "            get_final_output(pred, unique_intent)\n",
    "\n",
    "        \n",
    "cli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
