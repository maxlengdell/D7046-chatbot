{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    df = pd.read_csv(filename, encoding = \"latin1\", names = [\"Sentence\", \"Intent\"])\n",
    "    print(df.head())\n",
    "    intent = df[\"Intent\"]\n",
    "    unique_intent = list(set(intent))\n",
    "    sentences = list(df[\"Sentence\"])\n",
    "  \n",
    "    return (intent, unique_intent, sentences)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Sentence   Intent\n",
      "0  How many <item> are left?  balance\n",
      "1           Stock of <Item>?  balance\n",
      "2        Do you have <Item>?  balance\n",
      "3         Balance of <Item>?  balance\n",
      "4           Any <Item> left?  balance\n"
     ]
    }
   ],
   "source": [
    "intent, unique_intent, sentences = load_dataset(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have you run out of  ?\n",
      "['balance', 'order', 'price', 'common']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[35])\n",
    "print(unique_intent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/maxlengdell/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/maxlengdell/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define stemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(sentences):\n",
    "    words = []\n",
    "    for s in sentences:\n",
    "        clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", s)\n",
    "        w = word_tokenize(clean)\n",
    "        #stemming\n",
    "        words.append([i.lower() for i in w])\n",
    "    \n",
    "    return words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235\n",
      "[['how', 'many', 'item', 'are', 'left'], ['stock', 'of', 'item']]\n"
     ]
    }
   ],
   "source": [
    "cleaned_words = cleaning(sentences)\n",
    "print(len(cleaned_words))\n",
    "print(cleaned_words[:2])  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(words, filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'):\n",
    "    token = Tokenizer(filters = filters)\n",
    "    token.fit_on_texts(words)\n",
    "    print(token)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(words):\n",
    "    return(len(max(words, key = len)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7fc3740946d8>\n",
      "Vocab Size = 145 and Maximum length = 9\n"
     ]
    }
   ],
   "source": [
    "word_tokenizer = create_tokenizer(cleaned_words)\n",
    "vocab_size = len(word_tokenizer.word_index) + 1\n",
    "max_length = max_length(cleaned_words)\n",
    "\n",
    "print(\"Vocab Size = %d and Maximum length = %d\" % (vocab_size, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_doc(token, words):\n",
    "    return(token.texts_to_sequences(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_doc = encoding_doc(word_tokenizer, cleaned_words)\n",
    "#print(encoded_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_doc(encoded_doc, max_length):\n",
    "    return(pad_sequences(encoded_doc, maxlen = max_length, padding = \"post\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_doc = padding_doc(encoded_doc, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7, 18,  1, 21, 29,  0,  0,  0,  0],\n",
       "       [11,  2,  1,  0,  0,  0,  0,  0,  0],\n",
       "       [19,  4, 14,  1,  0,  0,  0,  0,  0],\n",
       "       [46,  2,  1,  0,  0,  0,  0,  0,  0],\n",
       "       [33,  1, 29,  0,  0,  0,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_doc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded docs =  (235, 9)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of padded docs = \",padded_doc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7fc3736b3f60>\n"
     ]
    }
   ],
   "source": [
    "#tokenizer with filter changed\n",
    "output_tokenizer = create_tokenizer(unique_intent, filters = '!\"#$%&()*+,-/:;<=>?@[\\]^`{|}~')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'balance': 1, 'order': 2, 'price': 3, 'common': 4}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_output = encoding_doc(output_tokenizer, intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_output = np.array(encoded_output).reshape(len(encoded_output), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(encode):\n",
    "    o = OneHotEncoder(sparse = False)\n",
    "    return(o.fit_transform(encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_one_hot = one_hot(encoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 4)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_Y, val_Y = train_test_split(padded_doc, output_one_hot, shuffle = True, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X = (188, 9) and train_Y = (188, 4)\n",
      "Shape of val_X = (47, 9) and val_Y = (47, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train_X = %s and train_Y = %s\" % (train_X.shape, train_Y.shape))\n",
    "print(\"Shape of val_X = %s and val_Y = %s\" % (val_X.shape, val_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 128, input_length = max_length, trainable = True))\n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "#   model.add(LSTM(128))\n",
    "    model.add(Dense(32, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4, activation = \"softmax\"))\n",
    "  \n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 9, 128)            18560     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 290,084\n",
      "Trainable params: 290,084\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size, max_length)\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxlengdell/opt/anaconda3/envs/nnD7046E/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 188 samples, validate on 47 samples\n",
      "Epoch 1/200\n",
      "188/188 [==============================] - 4s 22ms/step - loss: 1.3761 - accuracy: 0.2926 - val_loss: 1.3755 - val_accuracy: 0.3191\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.37553, saving model to model.h5\n",
      "Epoch 2/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 1.3362 - accuracy: 0.3457 - val_loss: 1.3660 - val_accuracy: 0.2979\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.37553 to 1.36598, saving model to model.h5\n",
      "Epoch 3/200\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 1.3016 - accuracy: 0.3936 - val_loss: 1.3546 - val_accuracy: 0.3191\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.36598 to 1.35458, saving model to model.h5\n",
      "Epoch 4/200\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 1.2266 - accuracy: 0.4947 - val_loss: 1.3443 - val_accuracy: 0.4894\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.35458 to 1.34427, saving model to model.h5\n",
      "Epoch 5/200\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 1.2114 - accuracy: 0.4521 - val_loss: 1.2894 - val_accuracy: 0.3191\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.34427 to 1.28943, saving model to model.h5\n",
      "Epoch 6/200\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 1.0730 - accuracy: 0.6277 - val_loss: 1.1903 - val_accuracy: 0.6383\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.28943 to 1.19030, saving model to model.h5\n",
      "Epoch 7/200\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.8771 - accuracy: 0.6862 - val_loss: 0.9938 - val_accuracy: 0.6809\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.19030 to 0.99378, saving model to model.h5\n",
      "Epoch 8/200\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.6765 - accuracy: 0.7606 - val_loss: 0.8453 - val_accuracy: 0.7234\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.99378 to 0.84533, saving model to model.h5\n",
      "Epoch 9/200\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.4341 - accuracy: 0.8511 - val_loss: 0.8290 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.84533 to 0.82898, saving model to model.h5\n",
      "Epoch 10/200\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.2920 - accuracy: 0.8723 - val_loss: 1.1170 - val_accuracy: 0.6596\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.82898\n",
      "Epoch 11/200\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.2941 - accuracy: 0.8989 - val_loss: 0.9475 - val_accuracy: 0.7021\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.82898\n",
      "Epoch 12/200\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.1863 - accuracy: 0.9521 - val_loss: 0.6816 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.82898 to 0.68158, saving model to model.h5\n",
      "Epoch 13/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.1403 - accuracy: 0.9734 - val_loss: 0.7336 - val_accuracy: 0.8298\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.68158\n",
      "Epoch 14/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.1320 - accuracy: 0.9521 - val_loss: 1.0032 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.68158\n",
      "Epoch 15/200\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.9840 - val_loss: 1.2500 - val_accuracy: 0.7660\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.68158\n",
      "Epoch 16/200\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.0543 - accuracy: 0.9734 - val_loss: 1.4055 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.68158\n",
      "Epoch 17/200\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.0282 - accuracy: 0.9947 - val_loss: 1.3271 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.68158\n",
      "Epoch 18/200\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.0552 - accuracy: 0.9840 - val_loss: 1.5880 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.68158\n",
      "Epoch 19/200\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.0454 - accuracy: 0.9840 - val_loss: 1.7144 - val_accuracy: 0.7660\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.68158\n",
      "Epoch 20/200\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.9787 - val_loss: 1.5815 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.68158\n",
      "Epoch 21/200\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.0210 - accuracy: 0.9947 - val_loss: 1.6304 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.68158\n",
      "Epoch 22/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 1.6717 - val_accuracy: 0.7660\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.68158\n",
      "Epoch 23/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0306 - accuracy: 0.9894 - val_loss: 1.7533 - val_accuracy: 0.7660\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.68158\n",
      "Epoch 24/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 1.7958 - val_accuracy: 0.7660\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.68158\n",
      "Epoch 25/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 1.8719 - val_accuracy: 0.7660\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.68158\n",
      "Epoch 26/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 1.9574 - val_accuracy: 0.7660\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.68158\n",
      "Epoch 27/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.9947 - val_loss: 2.0368 - val_accuracy: 0.7660\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.68158\n",
      "Epoch 28/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0112 - accuracy: 0.9947 - val_loss: 2.0863 - val_accuracy: 0.7660\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.68158\n",
      "Epoch 29/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 2.1521 - val_accuracy: 0.7660\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.68158\n",
      "Epoch 30/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 2.2254 - val_accuracy: 0.7660\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.68158\n",
      "Epoch 31/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0159 - accuracy: 1.0000 - val_loss: 2.2576 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.68158\n",
      "Epoch 32/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 2.2426 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.68158\n",
      "Epoch 33/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 2.2500 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.68158\n",
      "Epoch 34/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 0.9947 - val_loss: 2.2955 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.68158\n",
      "Epoch 35/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 2.3891 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.68158\n",
      "Epoch 36/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0200 - accuracy: 0.9947 - val_loss: 2.4810 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.68158\n",
      "Epoch 37/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 2.5655 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.68158\n",
      "Epoch 38/200\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 2.6319 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.68158\n",
      "Epoch 39/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 2.6763 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.68158\n",
      "Epoch 40/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 0.9947 - val_loss: 2.7528 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.68158\n",
      "Epoch 41/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 2.7981 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.68158\n",
      "Epoch 42/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 2.8063 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.68158\n",
      "Epoch 43/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 2.7648 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.68158\n",
      "Epoch 44/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 2.7279 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.68158\n",
      "Epoch 45/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 2.7488 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.68158\n",
      "Epoch 46/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 0.9894 - val_loss: 2.7419 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.68158\n",
      "Epoch 47/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 2.6210 - val_accuracy: 0.7660\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.68158\n",
      "Epoch 48/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.6923 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.68158\n",
      "Epoch 49/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.9947 - val_loss: 2.7506 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.68158\n",
      "Epoch 50/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 2.7995 - val_accuracy: 0.7234\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.68158\n",
      "Epoch 51/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 2.8405 - val_accuracy: 0.7234\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.68158\n",
      "Epoch 52/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 2.8764 - val_accuracy: 0.7234\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.68158\n",
      "Epoch 53/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 2.9043 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.68158\n",
      "Epoch 54/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 0.9947 - val_loss: 2.9075 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.68158\n",
      "Epoch 55/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 2.9230 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.68158\n",
      "Epoch 56/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 3.0070 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.68158\n",
      "Epoch 57/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 3.1057 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.68158\n",
      "Epoch 58/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0202 - accuracy: 0.9947 - val_loss: 3.1370 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.68158\n",
      "Epoch 59/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 2.9936 - val_accuracy: 0.7234\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.68158\n",
      "Epoch 60/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 2.9950 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.68158\n",
      "Epoch 61/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 0.9947 - val_loss: 3.0170 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.68158\n",
      "Epoch 62/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.0688 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.68158\n",
      "Epoch 63/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.1103 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.68158\n",
      "Epoch 64/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 3.1268 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.68158\n",
      "Epoch 65/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 3.1397 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.68158\n",
      "Epoch 66/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 3.1081 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.68158\n",
      "Epoch 67/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 3.0766 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.68158\n",
      "Epoch 68/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 2.9272 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.68158\n",
      "Epoch 69/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 0.9947 - val_loss: 2.8091 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.68158\n",
      "Epoch 70/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.7763 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.68158\n",
      "Epoch 71/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.8204 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.68158\n",
      "Epoch 72/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 2.8769 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.68158\n",
      "Epoch 73/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0093 - accuracy: 0.9947 - val_loss: 2.9483 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.68158\n",
      "Epoch 74/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0162 - accuracy: 0.9947 - val_loss: 3.0137 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.68158\n",
      "Epoch 75/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 0.9947 - val_loss: 3.0590 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.68158\n",
      "Epoch 76/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 0.9947 - val_loss: 3.1453 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.68158\n",
      "Epoch 77/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 0.9947 - val_loss: 3.1697 - val_accuracy: 0.7660\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.68158\n",
      "Epoch 78/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.1698 - val_accuracy: 0.7660\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.68158\n",
      "Epoch 79/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 3.2194 - val_accuracy: 0.7660\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.68158\n",
      "Epoch 80/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 3.2886 - val_accuracy: 0.7660\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.68158\n",
      "Epoch 81/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 3.3707 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.68158\n",
      "Epoch 82/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.4613 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.68158\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.5016 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.68158\n",
      "Epoch 84/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 0.9947 - val_loss: 3.4773 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.68158\n",
      "Epoch 85/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.4700 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.68158\n",
      "Epoch 86/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0099 - accuracy: 0.9947 - val_loss: 3.4597 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.68158\n",
      "Epoch 87/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0175 - accuracy: 0.9947 - val_loss: 3.3906 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.68158\n",
      "Epoch 88/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 3.3650 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.68158\n",
      "Epoch 89/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9947 - val_loss: 3.3340 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.68158\n",
      "Epoch 90/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.9947 - val_loss: 3.3219 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.68158\n",
      "Epoch 91/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 3.3394 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.68158\n",
      "Epoch 92/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 0.9947 - val_loss: 3.3873 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.68158\n",
      "Epoch 93/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.4182 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.68158\n",
      "Epoch 94/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 3.4355 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.68158\n",
      "Epoch 95/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0099 - accuracy: 0.9947 - val_loss: 3.4748 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.68158\n",
      "Epoch 96/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 3.5322 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.68158\n",
      "Epoch 97/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.5780 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.68158\n",
      "Epoch 98/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9947 - val_loss: 3.6162 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.68158\n",
      "Epoch 99/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.6488 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.68158\n",
      "Epoch 100/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 3.6946 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.68158\n",
      "Epoch 101/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.7304 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.68158\n",
      "Epoch 102/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 6.2088e-04 - accuracy: 1.0000 - val_loss: 3.7301 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.68158\n",
      "Epoch 103/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 0.9947 - val_loss: 3.4546 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.68158\n",
      "Epoch 104/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 3.3442 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.68158\n",
      "Epoch 105/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 3.3218 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.68158\n",
      "Epoch 106/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 3.3464 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.68158\n",
      "Epoch 107/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 3.4026 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.68158\n",
      "Epoch 108/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.4613 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.68158\n",
      "Epoch 109/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 3.5162 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.68158\n",
      "Epoch 110/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 2.7474e-04 - accuracy: 1.0000 - val_loss: 3.5712 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.68158\n",
      "Epoch 111/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 0.9947 - val_loss: 3.6276 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.68158\n",
      "Epoch 112/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.6960 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.68158\n",
      "Epoch 113/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 2.4844e-04 - accuracy: 1.0000 - val_loss: 3.7474 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.68158\n",
      "Epoch 114/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 3.8040 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.68158\n",
      "Epoch 115/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 3.8715 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.68158\n",
      "Epoch 116/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 7.5818e-04 - accuracy: 1.0000 - val_loss: 3.9331 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.68158\n",
      "Epoch 117/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.9894 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.68158\n",
      "Epoch 118/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0126 - accuracy: 0.9947 - val_loss: 4.0493 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.68158\n",
      "Epoch 119/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.1229 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.68158\n",
      "Epoch 120/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.1755 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.68158\n",
      "Epoch 121/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 4.2228 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.68158\n",
      "Epoch 122/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 4.2497 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.68158\n",
      "Epoch 123/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 0.9947 - val_loss: 4.2523 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.68158\n",
      "Epoch 124/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 4.1304e-04 - accuracy: 1.0000 - val_loss: 4.2509 - val_accuracy: 0.7447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00124: val_loss did not improve from 0.68158\n",
      "Epoch 125/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 4.2201 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.68158\n",
      "Epoch 126/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 4.0868 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.68158\n",
      "Epoch 127/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 0.9947 - val_loss: 4.0200 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.68158\n",
      "Epoch 128/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 4.1854e-04 - accuracy: 1.0000 - val_loss: 4.0109 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.68158\n",
      "Epoch 129/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.0403 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.68158\n",
      "Epoch 130/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 4.0793 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.68158\n",
      "Epoch 131/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 4.1598 - val_accuracy: 0.7447\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.68158\n",
      "Epoch 132/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0529 - accuracy: 0.9840 - val_loss: 4.3111 - val_accuracy: 0.7021\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.68158\n",
      "Epoch 133/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.1218 - accuracy: 0.9787 - val_loss: 2.7202 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.68158\n",
      "Epoch 134/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.1183 - accuracy: 0.9840 - val_loss: 1.7121 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.68158\n",
      "Epoch 135/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0303 - accuracy: 0.9894 - val_loss: 1.4250 - val_accuracy: 0.7660\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.68158\n",
      "Epoch 136/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0155 - accuracy: 0.9947 - val_loss: 1.4383 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.68158\n",
      "Epoch 137/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.4510 - val_accuracy: 0.7660\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.68158\n",
      "Epoch 138/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0179 - accuracy: 0.9947 - val_loss: 1.4594 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.68158\n",
      "Epoch 139/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 1.4660 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.68158\n",
      "Epoch 140/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 1.4646 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.68158\n",
      "Epoch 141/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0211 - accuracy: 0.9894 - val_loss: 1.4504 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.68158\n",
      "Epoch 142/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 1.4385 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.68158\n",
      "Epoch 143/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 1.4329 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.68158\n",
      "Epoch 144/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0116 - accuracy: 0.9947 - val_loss: 1.4384 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.68158\n",
      "Epoch 145/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0093 - accuracy: 0.9947 - val_loss: 1.4640 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.68158\n",
      "Epoch 146/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0081 - accuracy: 0.9947 - val_loss: 1.4926 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.68158\n",
      "Epoch 147/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.5065 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.68158\n",
      "Epoch 148/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.5166 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.68158\n",
      "Epoch 149/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 0.9947 - val_loss: 1.5286 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.68158\n",
      "Epoch 150/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 0.9947 - val_loss: 1.5396 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.68158\n",
      "Epoch 151/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0197 - accuracy: 0.9894 - val_loss: 1.5587 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.68158\n",
      "Epoch 152/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 1.5829 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.68158\n",
      "Epoch 153/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.6132 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.68158\n",
      "Epoch 154/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 0.9947 - val_loss: 1.6315 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.68158\n",
      "Epoch 155/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.6354 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.68158\n",
      "Epoch 156/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.6500 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.68158\n",
      "Epoch 157/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 1.6668 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.68158\n",
      "Epoch 158/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 1.6735 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.68158\n",
      "Epoch 159/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 1.6801 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.68158\n",
      "Epoch 160/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 6.5177e-04 - accuracy: 1.0000 - val_loss: 1.6854 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.68158\n",
      "Epoch 161/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.6910 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.68158\n",
      "Epoch 162/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.7007 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.68158\n",
      "Epoch 163/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0124 - accuracy: 0.9947 - val_loss: 1.7106 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.68158\n",
      "Epoch 164/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 1.7153 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.68158\n",
      "Epoch 165/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.7201 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.68158\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 1.7271 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.68158\n",
      "Epoch 167/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 0.9947 - val_loss: 1.7161 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.68158\n",
      "Epoch 168/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.7183 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.68158\n",
      "Epoch 169/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0133 - accuracy: 0.9947 - val_loss: 1.7244 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.68158\n",
      "Epoch 170/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 0.9947 - val_loss: 1.7363 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.68158\n",
      "Epoch 171/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 1.7479 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.68158\n",
      "Epoch 172/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.7633 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.68158\n",
      "Epoch 173/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.7797 - val_accuracy: 0.8085\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.68158\n",
      "Epoch 174/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0096 - accuracy: 0.9947 - val_loss: 1.8111 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.68158\n",
      "Epoch 175/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 1.8582 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.68158\n",
      "Epoch 176/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 0.9947 - val_loss: 1.9309 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.68158\n",
      "Epoch 177/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.9908 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.68158\n",
      "Epoch 178/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 7.3098e-04 - accuracy: 1.0000 - val_loss: 2.0229 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.68158\n",
      "Epoch 179/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.0454 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.68158\n",
      "Epoch 180/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 2.0652 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.68158\n",
      "Epoch 181/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 2.0862 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.68158\n",
      "Epoch 182/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 0.9947 - val_loss: 2.0991 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.68158\n",
      "Epoch 183/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 2.1179 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.68158\n",
      "Epoch 184/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.1317 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.68158\n",
      "Epoch 185/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 4.1382e-04 - accuracy: 1.0000 - val_loss: 2.1355 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.68158\n",
      "Epoch 186/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0124 - accuracy: 0.9947 - val_loss: 2.1492 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.68158\n",
      "Epoch 187/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.1694 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.68158\n",
      "Epoch 188/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0128 - accuracy: 0.9947 - val_loss: 2.2252 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.68158\n",
      "Epoch 189/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9947 - val_loss: 2.2657 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.68158\n",
      "Epoch 190/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 1.9880e-04 - accuracy: 1.0000 - val_loss: 2.2949 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.68158\n",
      "Epoch 191/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.3107 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.68158\n",
      "Epoch 192/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.3219 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.68158\n",
      "Epoch 193/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9947 - val_loss: 2.3278 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.68158\n",
      "Epoch 194/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 2.0713e-04 - accuracy: 1.0000 - val_loss: 2.3317 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.68158\n",
      "Epoch 195/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0293 - accuracy: 0.9734 - val_loss: 2.3158 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.68158\n",
      "Epoch 196/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0096 - accuracy: 0.9947 - val_loss: 2.3023 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.68158\n",
      "Epoch 197/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 9.8299e-04 - accuracy: 1.0000 - val_loss: 2.2985 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.68158\n",
      "Epoch 198/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 0.9894 - val_loss: 2.3507 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.68158\n",
      "Epoch 199/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 0.9947 - val_loss: 2.4567 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.68158\n",
      "Epoch 200/200\n",
      "188/188 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.5102 - val_accuracy: 0.7872\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.68158\n"
     ]
    }
   ],
   "source": [
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "history = model.fit(train_X, train_Y, epochs = 200, batch_size = 32, validation_data = (val_X, val_Y), callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxlengdell/opt/anaconda3/envs/nnD7046E/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    " model = load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(text):\n",
    "    clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", text)\n",
    "    test_word = word_tokenize(clean)\n",
    "    test_word = [w.lower() for w in test_word]\n",
    "    test_ls = word_tokenizer.texts_to_sequences(test_word)\n",
    "    print(test_word)\n",
    "    #Check for unknown words\n",
    "    if [] in test_ls:\n",
    "        test_ls = list(filter(None, test_ls))\n",
    "    \n",
    "    test_ls = np.array(test_ls).reshape(1, len(test_ls))\n",
    " \n",
    "    x = padding_doc(test_ls, max_length)\n",
    "  \n",
    "    pred = model.predict_proba(x)\n",
    "  \n",
    "  \n",
    "    return pred\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_output(pred, classes):\n",
    "    predictions = pred[0]\n",
    " \n",
    "    classes = np.array(classes)\n",
    "    ids = np.argsort(-predictions)\n",
    "    classes = classes[ids]\n",
    "    predictions = -np.sort(-predictions)\n",
    " \n",
    "    for i in range(pred.shape[1]):\n",
    "        print(\"%s has confidence = %s\" % (classes[i], (predictions[i])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'much', 'does', 'it', 'cost']\n",
      "price has confidence = 1.0\n",
      "order has confidence = 8.161783e-11\n",
      "common has confidence = 8.893908e-17\n",
      "balance has confidence = 6.5078595e-19\n"
     ]
    }
   ],
   "source": [
    "#Price\n",
    "text = \"How much does it cost?\"\n",
    "pred = predictions(text)\n",
    "get_final_output(pred, unique_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'many', 'pants', 'are', 'in', 'stock']\n",
      "balance has confidence = 1.0\n",
      "common has confidence = 2.8766227e-09\n",
      "order has confidence = 1.1516101e-11\n",
      "price has confidence = 1.068425e-14\n"
     ]
    }
   ],
   "source": [
    "#Balance\n",
    "text = \"how many pants are in stock?\"\n",
    "pred = predictions(text)\n",
    "get_final_output(pred, unique_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'want', 'to', 'buy', '3', 'pants']\n",
      "order has confidence = 1.0\n",
      "balance has confidence = 1.7245578e-09\n",
      "common has confidence = 4.570789e-11\n",
      "price has confidence = 1.1538659e-11\n"
     ]
    }
   ],
   "source": [
    "#Order\n",
    "text = \"I want to buy 3 pants\"\n",
    "pred = predictions(text)\n",
    "get_final_output(pred, unique_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'need', 'help']\n",
      "common has confidence = 0.88086003\n",
      "balance has confidence = 0.08307988\n",
      "price has confidence = 0.022841835\n",
      "order has confidence = 0.013218205\n"
     ]
    }
   ],
   "source": [
    "#Common\n",
    "text = \"I need help\"\n",
    "pred = predictions(text)\n",
    "get_final_output(pred, unique_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "opening_lines = ['Hello, what can I help you with?',\n",
    "                'How can I help you?',\n",
    "                'Whats on your mind today?']\n",
    "\n",
    "def randomOpening():\n",
    "    index = random.randint(0,len(opening_lines)-1)\n",
    "    print(opening_lines[index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, what can I help you with?\n"
     ]
    }
   ],
   "source": [
    "def cli():\n",
    "    while True:\n",
    "        randomOpening()\n",
    "        user_input = input(\"Ask the chatbot something: \")\n",
    "        print(user_input)\n",
    "        if(user_input==\"quit\"):\n",
    "            break\n",
    "        else:\n",
    "            pred = predictions(user_input)\n",
    "            print(pred)\n",
    "            get_final_output(pred, unique_intent)\n",
    "\n",
    "        \n",
    "cli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
